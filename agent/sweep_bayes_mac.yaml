project: ppo-hyperparameter-sweep
entity: self-play-project
program: ppo.py
command:
  - ${env}
  - caffeinate
  - "-s"
  - python
  - ${program}
  - "--cpu"
  - 2
  - "--video"
  - "True"
  - "--epochs"
  - 150
  - "--sweep"
  - "True"
  - "--env_list"
  - "LunarLanderContinuous-v2"
  - "Ant-v3"
  - "quadruped run"
  - "cheetah run"
  - "walker run"
  - "humanoid run"
  - "walker stand"
  - "walker walk"
  - "hopper hop"
  - "cartpole balance"
  - ${args}
method: bayes
metric:
  name: eval_returns
  goal: maximize
parameters:
  activation:
    values: ["tanh", "relu"]
  pi_width: 
    values: [32, 64, 128, 256, 512]
  pi_depth: 
    values: [1, 2, 4, 8]
  vf_width: 
    values: [32, 64, 128]
  vf_depth: 
    values: [1, 2, 4, 8]
  pi_weight_ratio:
    values: [1, 0.01, 0]
  pi_input_norm:
    values: [True, False]
  vf_input_norm:
    values: [False]
  
  std_dim:
    values: [0, 1]
  std_value:
    min: 0.1
    max: 1
  std_source:
    values: ["Network", "Parameter", "Constant"]

  seed:
    min: 0
    max: 1000000
    distribution: int_uniform
  squash:
    values: [True, False]
  squash_mean:
    values: [True, False]
  gamma:
    min: 0.99
    max: 0.999
  clip_ratio:
    min: .15
    max: .25
  pi_lr:
    min: 0.0001
    max: 0.0005
  pi_decay:
    min: 0.9
    max: 1.0
  vf_lr:
    min: 0.0001
    max: 0.002
  lam:
    min: .96
    max: .99
  
  stop_metric:
    values: ["None", "None", "None", "Entropy", "KL", "Reference KL"]
  min_entropy:
    values: [0, 0.01, 0.1]
  max_kl:
    values: [0.01, 0.02, 0.05, 0.5, 3]
  max_rev_kl:
    values: [0.01, 0.02, 0.05, 0.5, 3]
  max_ref_kl:
    values: [0.5, 1, 3, 10]
  
  reg_metric:
    values: ["None", "None", "None", "Entropy", "KL", "Reference KL"]
  entropy_coeff: 
    values: [0, 0.01, 0.2]
  kl_coeff:
    values: [0, 0.01, 0.2]
  rev_kl_coeff:
    values: [0, 0.01, 0.2]
  ref_kl_coeff:
    values: [0, 0.0001, 0.01]


